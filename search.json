[{"title":"python爬虫-urllib库","url":"https://vbozhang.github.io/2021/02/04/python-crawler/crawler1-urllib/","content":"网络爬虫系列的第二篇，使用python自带的urllib库开发爬虫\n\n\n urllib库简介\n","categories":["python"],"tags":["网络爬虫","urllib","python"]},{"title":"python爬虫-了解爬虫与http","url":"https://vbozhang.github.io/2021/02/02/python-crawler/basics/","content":"决定系统学习下网络爬虫，这是这个系列的第一篇，主要是爬虫相关的一些概念的介绍\n\n\n 网络爬虫\n 什么是网络爬虫\n网络爬虫，又被称为网页蜘蛛，网络机器人，在FOAF（FOAF，即Friend-of-a-Friend，简称FOAF。FOAF 是一种 XML/RDF 词汇表，它以计算机可读的形式描述您通常可能放在主 Web 页面上的个人信息之类的信息）社区中间，更经常的称为网页追逐者，是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本，另外一些不常使用的名字还有蚂蚁、自动索引、模拟程序或者蠕虫。\n\n更通俗一点说\n爬虫就是一个探测机器，它的基本操作就是模拟人的行为去各个网站溜达，点点按钮，查查数据，或者把看到的信息背回来。就像一只虫子在一幢楼里不知疲倦地爬来爬去。\n你可以简单地想象：每个爬虫都是你的「分身」。就像孙悟空拔了一撮汗毛，吹出一堆猴子一样。\n你每天使用的百度，其实就是利用了这种爬虫技术：每天放出无数爬虫到各个网站，把他们的信息抓回来，然后化好淡妆排着小队等你来检索。\n抢票软件，就相当于撒出去无数个分身，每一个分身都帮助你不断刷新 12306 网站的火车余票。一旦发现有票，就马上拍下来，然后对你喊：土豪快来付款。\n\n 网络爬虫的分类\n根据使用场景，爬虫可以分为通用爬虫和聚焦爬虫\n\n通用爬虫：通用爬虫是搜索引擎抓取系统（百度、谷歌、搜狗等）的重要组成部分。主要是将互联网上的网页下载到本地，形成一个互联网内容的镜像备份。\n聚焦爬虫：是面向特定需求的一种网络爬虫程序，他与通用爬虫的区别在于：聚焦爬虫在实施网页抓取的时候会对内容进行筛选和处理，尽量保证只抓取与需求相关的网页信息。\n\n 为什么用python写网络爬虫\n爬虫是工具性程序，对速度和效率要求高，PHP对对线程、异步支持不是很好，并发处理能力弱；JAVA语言笨重，代码量大，重构成本高；C/C++学习和开发成本高；Python代码简洁，支持模块多，开发效率高，易于修改，适合作为开发爬虫的语言。\n http协议\nhttp协议：超文本传输协议（英文：HyperText Transfer Protocol，缩写：HTTP）是一种用于分布式、协作式和超媒体信息系统的应用层协议。HTTP是万维网的数据通信的基础。\n http工作原理\n以下是 HTTP 请求/响应的步骤：\n\n客户端连接到Web服务器\n一个HTTP客户端，通常是浏览器，与Web服务器的HTTP端口（默认为80）建立一个TCP套接字连接。例如，http://www.baidu.com。\n（注意：http协议默认端口为80，https协议默认端口为443）\n发送HTTP请求\n通过TCP套接字，客户端向Web服务器发送一个文本的请求报文，一个请求报文由请求行、请求头部、空行和请求数据4部分组成。\n服务器接受请求并返回HTTP响应\nWeb服务器解析请求，定位请求资源。服务器将资源复本写到TCP套接字，由客户端读取。一个响应由状态行、响应头部、空行和响应数据4部分组成。\n释放连接TCP连接\n若connection 模式为close，则服务器主动关闭TCP连接，客户端被动关闭连接，释放TCP连接;若connection 模式为keepalive，则该连接会保持一段时间，在该时间内可以继续接收请求;\n客户端浏览器解析HTML内容\n客户端浏览器首先解析状态行，查看表明请求是否成功的状态代码。然后解析每一个响应头，响应头告知以下为若干字节的HTML文档和文档的字符集。客户端浏览器读取响应数据HTML，根据HTML的语法对其进行格式化，并在浏览器窗口中显示。\n\n例如：在浏览器地址栏键入URL，按下回车之后会经历以下流程：\n\n浏览器向 DNS 服务器请求解析该 URL 中的域名所对应的 IP 地址;\n解析出 IP 地址后，根据该 IP 地址和默认端口 80，和服务器建立TCP连接;\n浏览器发出读取文件(URL 中域名后面部分对应的文件)的HTTP 请求，该请求报文作为 TCP 三次握手的第三个报文的数据发送给服务器;\n服务器对浏览器请求作出响应，并把对应的 html 文本发送给浏览器;\n释放 TCP连接;\n浏览器将该 html 文本并显示内容;\n\n URL\nURL是Uniform Resource Locator的简写，即统一资源定位器\nURL的组成：\n\n\nprotocol：协议，例如：http\nhostname：主机地址，可以是域名，也可以是IP地址，例如：www.baidu.com\nport；端口,http协议默认端口是：80端口\npath:路径,网络资源在服务器中的指定路径,例如：cajn.cnki.net/xgbt，xgbt就是path\nparameter:参数,如果要向服务器传入参数，在这部分输入\nquery:查询字符串,如果需要从服务器那里查询内容，在这里编辑,例如：www.baidu.com/s?wd=python，这里的wd=python就是查询字符串\nfragment:片段,网页中可能会分为不同的片段，如果想访问网页后直接到达指定位置，可以在这部分设置，例子的话可以参考百度百科通过目录跳转的方法，会在URL后面加上#1，#2等，这是前端用来页面定位的，后台一般不用管\n\n我们在浏览器上输入网址的时候并没有这么复杂，那是因为浏览器会对内容进行自动补全；在浏览器请求一个URL，浏览器回对URL进行编码，除英文，数字和部分符号外，其他的全部使用百分号+十六进制码值进行编码。\n HTTP请求协议\n 请求方法\n请求数据有多种方法，常用的有GET和POST\n\n\n\n序号\n方法\n描述\n\n\n\n\n1\nGET\n请求指定的页面信息，并返回实体主体。\n\n\n2\nHEAD\n类似于 GET 请求，只不过返回的响应中没有具体的内容，用于获取报头\n\n\n3\nPOST\n向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST 请求可能会导致新的资源的建立和/或已有资源的修改。\n\n\n4\nPUT\n从客户端向服务器传送的数据取代指定的文档的内容。\n\n\n5\nDELETE\n请求服务器删除指定的页面。\n\n\n6\nCONNECT\nHTTP/1.1 协议中预留给能够将连接改为管道方式的代理服务器。\n\n\n7\nOPTIONS\n允许客户端查看服务器的性能。\n\n\n8\nTRACE\n回显服务器收到的请求，主要用于测试或诊断。\n\n\n9\nPATCH\n是对 PUT 方法的补充，用来对已知资源进行局部更新 。\n\n\n\n 响应状态码\n 请求收到，继续处理：\n\n100      客户端必须继续发出请求\n101      客户端要求服务器根据请求转换HTTP协议版本\n\n 操作成功收到，分析，接受：\n\n200      交易成功\n201      提示知道新文件的URL\n202      接受和处理、但处理未完成\n203      返回信息不确定或不完整\n204      请求收到，但返回信息为空\n205      服务器完成了请求，用户代理必须复位当前已经浏览过的文件\n206      服务器已经完成了部分用户的GET请求\n\n 重定向：\n\n300      请求的资源可在多处得到\n301      永久重定向，在Location响应首部的值仍为当前URL(隐式重定向)\n302      临时重定向，在Location响应首部的值仍为新的URL(显示重定向)\n303      建议客户端访问其他URL或访问方式\n304      Not Modified 请求的资源没有改变 可以继续使用缓存\n305      请求的资源必须从服务器指定的地址得到\n306      前一版本HTTP中使用的代码，现行版本中不再使用\n307      声明请求的资源临时性删除\n\n 客户端错误：\n\n400      错误请求，如语法错误\n401      未授权\nHTTP 401.1    未授权，登录失败\nHTTP 401.2    未授权，服务器配置问题导致登录失败\nHTTP 401.3    ACL  禁止访问资源\nHTTP 401.4    未授权  授权被筛选器拒绝\nHTTP 401.5    未授权  ISAPI或CGI授权失败\n402      保留有效ChargeTo头响应\n403      禁止访问\nHTTP 403.1    禁止访问  禁止可执行访问\nHTTP 403.2    禁止访问  禁止读访问\nHTTP 403.3    禁止访问  禁止写访问\nHTTP 403.4    禁止访问  要求SSL\nHTTP 403.5    禁止访问  要求SSL 128\nHTTP 403.6    禁止访问  IP地址被拒绝\nHTTP 403.7    禁止访问  要求客户端证书\nHTTP 403.8    禁止访问  禁止站点访问\nHTTP 403.9    禁止访问  连接的用户过多\nHTTP 403.10   禁止访问  配置无效\nHTTP 403.11   禁止访问  密码更改\nHTTP 403.12   禁止访问  映射器拒绝访问\nHTTP 403.13   禁止访问  客户端证书已被吊销\nHTTP 403.15   禁止访问  客户端访问许可过多\nHTTP 403.16   禁止访问  客户端证书不可信或者无效\nHTTP 403.17   禁止访问  客户端证书已经到期或者尚未生效\n404       没有发现文件、查询或URL\n405       用户在Request-Line字段定义的方法不允许\n406       根据用户发送的Accept拖，请求资源不可访问\n407       类似401，用户必须首先在代理服务器上得到授权\n408       客户端没有在用户指定的饿时间内完成请求\n409       对当前资源状态，请求不能完成\n410       服务器上不再有此资源且无进一步的参考地址\n411       服务器拒绝用户定义的Content-Length属性请求 \n412       一个或多个请求头字段在当前请求中错误\n413       请求的资源大于服务器允许的大小\n414       请求的资源URL长于服务器允许的长度\n415       请求资源不支持请求项目格式\n416       请求中包含Range请求头字段，在当前请求资源范围内没有range指示值，       请求也不包含If-Range请求头字段\n417       服务器不满足请求Expect头字段指定的期望值，如果是代理服务器，可能是下一级服务器不能满足请求长\n\n 服务器端错误:\n\n500 - 内部服务器错误\nHTTP 500.100       内部服务器错误 \nHTTP 500-11       服务器关闭\nHTTP 500-12       应用程序重新启动\nHTTP 500-13       服务器太忙\nHTTP 500-14       应用程序无效\nHTTP 500-15       不允许请求 \n501       未实现\n502       网关错误\n503       服务不可用\n504       网关超时\n\n 请求头参数\n在http协议中，向服务器发送一个请求，数据分为三部分，第一个会把数据放在URL中，第二个是在body中（post请求），第三个是放在head中，head即为请求头，爬虫常用请求头参数如下：\n\n\n\nHeader\n解释\n示例\n\n\n\n\nCookie\nHTTP请求发送时，会把保存在该请求域名下的所有cookie值一起发送给web服务器。\nCookie: $Version=1; Skin=new;\n\n\nReferer\n先前网页的地址，当前请求网页紧随其后，即来路\nReferer: http://www.example.com/archives…\n\n\nUser-Agent\nUser-Agent的内容包含发出请求的用户信息,包含浏览器名称\nUser-Agent: Mozilla/5.0 (Linux; X11)\n\n\n\n 浏览器开发者工具\nchrome浏览器的开发者工具可以在右键→检查调出，也可以f12或者\nCtrl+Shift+I（进入开发者工具）\nCtrl+Shift+J （进入开发者工具，并定位到控制台Console）\n 开发者工具简要说明：\n\nElements 元素，显示网页源码\nConsule 控制台，跟踪查看js代码\nSources 显示网页文件\nNetwork 查看网页请求\n\n 示例\n\n如图是开发者工具的界面，红框标注的是上文提到的http请求的相关参数。\n 爬虫使用时的法律要求\n法律方面参考《数据安全管理办法（征求意见稿）》，总之就是禁止恶意爬虫，所谓恶意爬虫就是涉及到以下方面\n\n侵犯著作权\n侵犯商业秘密\n侵犯个人隐私或个人信息\n构成不正当竞争\n侵入计算机系统，构成刑事犯罪\n\n同时遵守Robots协议，robots.txt协议并不是一个规范，而是约定俗成的东西，网站通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取：\n根据协议，网站管理员可以在网站域名的根目录下放一个robots.txt 文本文件，里面可以指定不同的网络爬虫能访问的页面和禁止访问的页面，指定的页面由正则表达式表示。网络爬虫在采集这个网站之前，首先获取到这个文件，然后解析到其中的规则，然后根据规则来采集网站的数据。\n TODO\n⛳到这里学习爬虫的铺垫就打好了，后面就从python中与爬虫相关的库开始学起\n\n   \n\n参考：\n[1] 21天搞定Python分布爬虫\n[2] 通俗的讲，网络爬虫到底是什么？.史中\n[3] HTTP协议超级详解.爱文飞翔\n[4] URL的组成格式.wudipmd\n[5] HTTP 请求方法.菜鸟教程\n[6] Apache之HTTP协议\n[7] Header:请求头参数详解.brucelong\n[8] 国家互联网信息办公室关于《数据安全管理办法（征求意见稿）》公开征求意见的通知\n[9] 网络爬虫的法律规制.2019.06.16\n","categories":["python"],"tags":["网络爬虫","python","http协议"]}]