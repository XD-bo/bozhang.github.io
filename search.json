[{"title":"CCS安装踩坑记录","url":"https://vbozhang.github.io/2021/03/17/electronics-design/CCSinstallation/","content":"对MSP430系列的单片机编程，最流行的是使用CCS集成开发环境，本文讲了CCS安装中踩过的坑\n\n\n\n因为老师这边TI家MSP430的单片机比较多，所以最早接触的单片机就是这款，优点和缺点就不说了，主要拿这个用来入门单片机就是了（据说掌握了一种单片机其他单片机入门也很快）\n在官方安装信息的页面可以下载软件，软件7.0以后的版本是免费的(7.0以前的版本也可以在这里找到许可证)，从官网上下载速度很快，下载好就可以解压安装了(我刚开始下载的是CCS10)\n安装失败，无法安装原因是字符集不正确(安装页面会有相关警告说明)安装说明上给出了建议\n\n由于CCS使用的Eclipse应用程序框架的限制，用户名不能包含任何Unicode字符。为了安装该产品，请创建一个没有任何Unicode字符的临时管理员用户，并为所有用户安装CCS\n\n就是采用新建一个用户的方法，保证新建的用户名只有ASCII字符(用户名全为英文就好了)，关于字符集和字符编码的介绍看这里，win10官方有介绍新建用户的方法（不建议注册表强制更改原用户名，会导致很多配置上的错误甚至文件丢失，早知道刚买电脑的时候就起一个英文的用户名了T_T）\n或者安装低版本的CCS就不用管这个可以直接安装，我当时就卸载了CCS10然后安装了CCS8，能够直接安装上(但最终运行的时候还会产生问题，下面会讲)\n软件无法打开软件第一次打开的时候没有遇到问题，但第二次打开的，时候出现错误如图这里的log路径在安装目录\\ccsv8\\eclipse\\configuration下，文件为一串数字+.log的格式，打开发现比较关键性的几句：\n!MESSAGE Error reading configuration: D:\\CSplus\\NUEDC\\CCS\\ccsv8\\eclipse\\configuration\\org.eclipse.osgi\\.manager\\.fileTableLock (拒绝访问。)\n!STACK 0\njava.io.FileNotFoundException: D:\\CSplus\\NUEDC\\CCS\\ccsv8\\eclipse\\configuration\\org.eclipse.osgi\\.manager\\.fileTableLock (拒绝访问。)\n\n大意就是对一些文件夹没有访问权限，那么以管理员身份运行就应该可以解决，为了以后打开方便，可以设置软件默认打开方式为以管理员方式运行:找到程序文件，右键→属性→兼容性→勾选以管理员方式运行至此，软件能够正常打开\n出现编译错误当打开工作区或编译文件时出现：TI论坛上有相关问题，就是安装路径和工作文件夹路径不能有Unicode字符，我当时确认了一下，确实没有Unicode字符，最后又查了半天，发现在C盘用户文件夹下会生成ti的相关临时文件，所以也用户名还是不能用中文，要按照上面介绍的方法新建一个用户名\n至此，安装完成，软件可以在新建用户登陆时正常使用\n","categories":["NUEDC"],"tags":["CSS","单片机","软件安装"]},{"title":"python爬虫-proxyhandler处理器","url":"https://vbozhang.github.io/2021/03/05/python-crawler/2ProxyHandler/","content":"网络爬虫系列的第三篇，使用proxyhandler处理器伪装IP\n\n\n\nProxyHandler处理器介绍IP地址，是网际协议中用于标识发送或接收数据报的设备的一串数字1，很多网站会检测 某一段时间某个IP的访问次数，如果访问过分频繁，可能会限制该IP的访问，所以我们需要借助某种方式来伪装我们的IP，使爬虫程序的正常运行。伪装IP的一种有效方式就是使用代理，其基本原理如下图：\n\n使用代理服务器访问网站，网站检测到的是代理服务器的IP，在代理服务器IP被禁用后可以更换新的代理服务器，进而可以实现伪装IP的目的。\n\n1.维基百科.IP地址. ↩","categories":["python"],"tags":["网络爬虫","urllib","python","IP代理"]},{"title":"python爬虫-文件读取","url":"https://vbozhang.github.io/2021/03/01/python-crawler/FileReading/","content":"网络爬虫系列的附加内容，对爬取到的内容进行读取操作\n\n\n\n\nread()方法read方法有一个参数：\nf.read(size) # f为文件对象\n可选参数size为数字，表示从已打开文件中读取的字节计数，默认情况下为读取全部示例：example.txt ：\n1 2 3\n4 5 6\n7 8 9\n\nwith  open('example.txt') as f:\n    content = f.read(6)\n    print(content)\n运行结果：\n1 2 3\n \n这里空格和回车各算一个字符，共读取6个。\nreadline()方法readline方法可以从文件中读取整行，包括换行符’\\n’\nf.readline(size)\nsize表示读取的字节数，默认情况下为读取一行中的全部字符示例：\nwith  open('example.txt') as f:\n    content = f.readline()\n    print(content)\n运行结果：\n1 2 3\n \nreadline方法会记住上一个readline函数读取的位置，接着读取下一行\nreadlines方法可以读取多行，没有参数，返回的是所有行组成的列表\n示例：\nwith  open('example.txt') as f:\n    content = f.readlines()\n    print(content)\n运行结果：\n[&#39;1 2 3\\n&#39;, &#39;4 5 6\\n&#39;, &#39;7 8 9&#39;]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n参考： 一文搞懂Python文件读写.知乎.朱卫军.\n\n\n\n调整本文代码样式： 明亮  黑暗 \n\n  function changePrismTheme(e) {\n    let text = e.target.innerHTML;\n    let linkList = document.head.getElementsByTagName('link');\n    for(let item of linkList) {\n      if(item.dataset.prism) {\n        document.head.removeChild(item);\n      }\n    }\n    let link = document.createElement('link');\n    link.rel = 'stylesheet';\n    link.dataset.prism = text;\n    if(text === '明亮') {\n      text = 'prism';\n    } else {\n      text = 'prism-tomorrow';\n    }\n    link.href='/js/lib/prism/' + text +'.min.css';\n    document.head.appendChild(link);\n  }\n  setTimeout(() => {\n    let buttonList = document.getElementsByClassName('postbutton');\n    Array.prototype.forEach.call(buttonList, item => {\n      item.onclick = changePrismTheme;\n    });\n  }, 0);\n\n\n\n\n","categories":["python"],"tags":["python","文件读取"]},{"title":"python爬虫-urllib库","url":"https://vbozhang.github.io/2021/02/04/python-crawler/1urllib/","content":"网络爬虫系列的第二篇，使用python自带的urllib库开发爬虫\n\n这里只做简单的介绍，以便后续使用\n\nurllib库简介\nurllib 是一个 Python 内置包，不需要额外安装即可使用，包里面包含了几个用来处理 url 的模块,是一个最基本的网络请求库，可以模拟浏览器的行为，向指定的服务器发送一个请求，并可以保存服务器返回的数据。1\n\n\n urllib.request，用来打开和读取 url，意思就是可以用它来模拟发送请求，就像在浏览器里输入网址然后敲击回车一样，获取网页响应内容。\n urllib.error，用来处理 urllib.request 引起的异常，保证程序的正常执行。\n urllib.parse，用来解析 url，可以对 url 进行拆分、合并等。\nurllib.robotparse，用来解析 robots.txt 文件，判断网站是否能够进行爬取。2\n\nurllib.request 模块request.urlopen 函数urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None)\n\n\n url，必选参数，是一个 str 字符串或者Request 对象\n data，如果传递 data 参数，urlopen 将使用 HTTP POST 方式请求，否则为 HTTP GET 请求。\ntimeout，可选参数，设置超时时间(未设置时使用全局默认超时时间)，以秒为单位计时，若请求超出了设置时间还未得到响应则抛出异常。\n剩余的参数不常用，不再详细介绍\n\n示例:3\nfrom urllib import request\n\nresp = request.urlopen(\"http://www.baidu.com\")\nprint(resp.read())\n运行结果是输出 http://www.baidu.com 的源代码\n也可以通过这种方式获取状态码\nfrom urllib import request\n\nresp = request.urlopen(\"http://www.baidu.com\")\nprint(resp.getcode())\n\n运行结果：\n200\n\nrequest.urlretrieve 函数urlretrieve(url, filename=None, reporthook=None, data=None)\n\npython3中urllib.request模块提供的urlretrieve()函数。urlretrieve()方法直接将远程数据下载到本地。4\n\n\n参数url：下载链接地址\n参数filename：指定了保存本地路径（如果参数未指定，urllib会生成一个临时文件保存数据。）\n参数reporthook：是一个回调函数，当连接上服务器、以及相应的数据块传输完毕时会触发该回调，我们可以利用这个回调函数来显示当前的下载进度。\n参数data：返回一个包含两个元素的(filename, headers) 元组，filename 表示保存到本地的路径，header表示服务器的响应头\n\n示例：\nfrom urllib import request\n\nrequest.urlretrieve(\"http://www.baidu.com\",\"baidu.html\")\n通过这个代码可以下载百度的网页到当前文件夹\nurllib.parse 模块parse.urlencode函数urlencode 可以把字典数据转换为 URL 编码的数据，用浏览器发送请求的时候，如果url 中包含了中文或者其他特殊字符，那么浏览器会自动编码，如果使用代码发送请求，那么就必须手动的进行编码\n示例：\nfrom urllib import parse\nparams = &#123;'name': '姓名', 'age': '18', 'greet': 'hello world'&#125;\nresult = parse.urlencode(params)\nprint(result)\n运行结果：\nname&#x3D;%E5%A7%93%E5%90%8D&amp;age&#x3D;18&amp;greet&#x3D;hello+world\n\nparse.parse_qs函数对URL进行解码，可以理解为urlencode的逆过程\nurlparse和urlsplit函数两个函数都用来对URL进行分割，两者用法相同，区别在于urlparse可以分割出URL的params属性，urlsplit则不能5示例：\nfrom urllib import parse\nurl = 'http://www.baidu.com/s?wd=demo&amp;time=2021#3'\nresult = parse.urlsplit(url)\nprint(result)\nprint('scheme:',result.scheme)\nprint('netloc:',result.netloc)\nprint('path:',result.path)\nprint('query:',result.query)\n运行结果：\nSplitResult(scheme='http', netloc='www.baidu.com', path='/s', query='wd=demo&amp;time=2021', fragment='3')\nscheme: http\nnetloc: www.baidu.com\npath: /s\nquery: wd=demo&amp;time=2021\n\nRequest类class urllib.request.Request(url, data=None, headers=&#123;&#125;, origin_req_host=None, unverifiable=False, method=None)\n\nRequest类是request下一个非常强大的类，用于在请求的时候加上请求头，例如User-Agent，以及POST请求所需要的data \n示例：\nfrom urllib import request, parse\n\nurl = 'https://www.lagou.com/jobs/positionAjax.json?needAddtionalResult=false'\n\nheaders = &#123;\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537'\n                  '.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36',\n    'Referer': 'https://www.lagou.com/jobs/list_python?labelWords=&amp;fromSearch=true&amp;suginput=',\n    'cookie': ''\n&#125;\n\ndata = &#123;\n    'first': 'ture',\n    'pn': 1,\n    'kd': 'python'\n&#125;\nreq = request.Request(url, headers=headers, data=parse.urlencode(data).encode('utf-8'), method='POST')\nresp = request.urlopen(req)\nprint(resp.read().decode('utf-8'))\n这段代码用于从“拉勾网”获取有关“python”的职位信息，拉勾网具有一定反爬机制，比较适合来做练习,关于代码的解释如下：\n\nURL、headers、 data 如何获取？\n  如下图，在拉勾网上搜索python，用f12或右键“检查”调出开发者工具，在Network上找到如下的JS文件，职位信息就在这个文件中，可以在Response页面查看职位信息，而URL、headers、 data的信息在Headers中，找的对应项补充在代码中即可\n\n\n\n\n为何在Request中要对data的格式进行处理？  data要先采用urlencode进行编码，但转化之后是unicode字符串，要转化成所支持的bytes数据类型，用encode进行转化\n\n最后输出时的decode作用是什么？  默认输出为bytes数据类型，用decode解码成易读的unicode类型并采用UTF-8编码\n\n注意：cookie信息这里并没有填，找到自己的cookie填上去即可运行\n\n\nTODO⛳urlllib库的简单介绍大概就这些了，接着是关于IP代理的设置与应用\n\n\n1.urllib — URL 处理模块 ↩2.urllib 包基本使用.纯洁的微笑 ↩3.关于read及其它文件读取的方法，参考我的这篇博客 ↩4.使用python urlretrieve下载文件.z小白. ↩5.例如http://www.baidu.com/s;demo1?wd=demo2 中的demo1只能被urlparse获取 ↩","categories":["python"],"tags":["网络爬虫","urllib","python"]},{"title":"python爬虫-了解爬虫与http","url":"https://vbozhang.github.io/2021/02/02/python-crawler/basics/","content":"决定系统学习下网络爬虫，这是这个系列的第一篇，主要是爬虫相关的一些概念的介绍\n\n\n\n网络爬虫什么是网络爬虫网络爬虫，又被称为网页蜘蛛，网络机器人，在FOAF（FOAF，即Friend-of-a-Friend，简称FOAF。FOAF 是一种 XML/RDF 词汇表，它以计算机可读的形式描述您通常可能放在主 Web 页面上的个人信息之类的信息）社区中间，更经常的称为网页追逐者，是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本，另外一些不常使用的名字还有蚂蚁、自动索引、模拟程序或者蠕虫。\n\n\n更通俗一点说\n爬虫就是一个探测机器，它的基本操作就是模拟人的行为去各个网站溜达，点点按钮，查查数据，或者把看到的信息背回来。就像一只虫子在一幢楼里不知疲倦地爬来爬去。你可以简单地想象：每个爬虫都是你的「分身」。就像孙悟空拔了一撮汗毛，吹出一堆猴子一样。你每天使用的百度，其实就是利用了这种爬虫技术：每天放出无数爬虫到各个网站，把他们的信息抓回来，然后化好淡妆排着小队等你来检索。抢票软件，就相当于撒出去无数个分身，每一个分身都帮助你不断刷新 12306 网站的火车余票。一旦发现有票，就马上拍下来，然后对你喊：土豪快来付款。\n\n\n网络爬虫的分类根据使用场景，爬虫可以分为通用爬虫和聚焦爬虫\n\n通用爬虫：通用爬虫是搜索引擎抓取系统（百度、谷歌、搜狗等）的重要组成部分。主要是将互联网上的网页下载到本地，形成一个互联网内容的镜像备份。\n聚焦爬虫：是面向特定需求的一种网络爬虫程序，他与通用爬虫的区别在于：聚焦爬虫在实施网页抓取的时候会对内容进行筛选和处理，尽量保证只抓取与需求相关的网页信息。\n\n为什么用python写网络爬虫爬虫是工具性程序，对速度和效率要求高，PHP对对线程、异步支持不是很好，并发处理能力弱；JAVA语言笨重，代码量大，重构成本高；C/C++学习和开发成本高；Python代码简洁，支持模块多，开发效率高，易于修改，适合作为开发爬虫的语言。\nhttp协议http协议：超文本传输协议（英文：HyperText Transfer Protocol，缩写：HTTP）是一种用于分布式、协作式和超媒体信息系统的应用层协议。HTTP是万维网的数据通信的基础。\nhttp工作原理以下是 HTTP 请求/响应的步骤：\n\n客户端连接到Web服务器一个HTTP客户端，通常是浏览器，与Web服务器的HTTP端口（默认为80）建立一个TCP套接字连接。例如，http://www.baidu.com。（注意：http协议默认端口为80，https协议默认端口为443）\n发送HTTP请求通过TCP套接字，客户端向Web服务器发送一个文本的请求报文，一个请求报文由请求行、请求头部、空行和请求数据4部分组成。\n服务器接受请求并返回HTTP响应Web服务器解析请求，定位请求资源。服务器将资源复本写到TCP套接字，由客户端读取。一个响应由状态行、响应头部、空行和响应数据4部分组成。\n释放连接TCP连接若connection 模式为close，则服务器主动关闭TCP连接，客户端被动关闭连接，释放TCP连接;若connection 模式为keepalive，则该连接会保持一段时间，在该时间内可以继续接收请求;\n客户端浏览器解析HTML内容客户端浏览器首先解析状态行，查看表明请求是否成功的状态代码。然后解析每一个响应头，响应头告知以下为若干字节的HTML文档和文档的字符集。客户端浏览器读取响应数据HTML，根据HTML的语法对其进行格式化，并在浏览器窗口中显示。\n\n例如：在浏览器地址栏键入URL，按下回车之后会经历以下流程：\n\n浏览器向 DNS 服务器请求解析该 URL 中的域名所对应的 IP 地址;\n解析出 IP 地址后，根据该 IP 地址和默认端口 80，和服务器建立TCP连接;\n浏览器发出读取文件(URL 中域名后面部分对应的文件)的HTTP 请求，该请求报文作为 TCP 三次握手的第三个报文的数据发送给服务器;\n服务器对浏览器请求作出响应，并把对应的 html 文本发送给浏览器;\n释放 TCP连接;\n浏览器将该 html 文本并显示内容; \n\nURLURL是Uniform Resource Locator的简写，即统一资源定位器\nURL的组成：\n\n\nprotocol：协议，例如：http\nhostname：主机地址，可以是域名，也可以是IP地址，例如：www.baidu.com\nport；端口,http协议默认端口是：80端口\npath:路径,网络资源在服务器中的指定路径,例如：cajn.cnki.net/xgbt，xgbt就是path\nparameter:参数,如果要向服务器传入参数，在这部分输入\nquery:查询字符串,如果需要从服务器那里查询内容，在这里编辑,例如：www.baidu.com/s?wd=python，这里的wd=python就是查询字符串\nfragment:片段,网页中可能会分为不同的片段，如果想访问网页后直接到达指定位置，可以在这部分设置，例子的话可以参考百度百科通过目录跳转的方法，会在URL后面加上#1，#2等，这是前端用来页面定位的，后台一般不用管\n\n我们在浏览器上输入网址的时候并没有这么复杂，那是因为浏览器会对内容进行自动补全；在浏览器请求一个URL，浏览器回对URL进行编码，除英文，数字和部分符号外，其他的全部使用百分号+十六进制码值进行编码。\n\nHTTP请求协议请求方法请求数据有多种方法，常用的有GET和POST\n\n\n\n序号\n方法\n描述\n\n\n\n1\n==GET==\n请求指定的页面信息，并返回实体主体。\n\n\n2\nHEAD\n类似于 GET 请求，只不过返回的响应中没有具体的内容，用于获取报头\n\n\n3\n==POST==\n向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST 请求可能会导致新的资源的建立和/或已有资源的修改。\n\n\n4\nPUT\n从客户端向服务器传送的数据取代指定的文档的内容。\n\n\n5\nDELETE\n请求服务器删除指定的页面。\n\n\n6\nCONNECT\nHTTP/1.1 协议中预留给能够将连接改为管道方式的代理服务器。\n\n\n7\nOPTIONS\n允许客户端查看服务器的性能。\n\n\n8\nTRACE\n回显服务器收到的请求，主要用于测试或诊断。\n\n\n9\nPATCH\n是对 PUT 方法的补充，用来对已知资源进行局部更新 。\n\n\n响应状态码请求收到，继续处理：\n100      客户端必须继续发出请求101      客户端要求服务器根据请求转换HTTP协议版本\n\n操作成功收到，分析，接受：\n==200      交易成功==201      提示知道新文件的URL202      接受和处理、但处理未完成203      返回信息不确定或不完整204      请求收到，但返回信息为空205      服务器完成了请求，用户代理必须复位当前已经浏览过的文件206      服务器已经完成了部分用户的GET请求\n\n重定向：\n300      请求的资源可在多处得到==301      永久重定向，在Location响应首部的值仍为当前URL(隐式重定向)====302      临时重定向，在Location响应首部的值仍为新的URL(显示重定向)==303      建议客户端访问其他URL或访问方式304      Not Modified 请求的资源没有改变 可以继续使用缓存305      请求的资源必须从服务器指定的地址得到306      前一版本HTTP中使用的代码，现行版本中不再使用307      声明请求的资源临时性删除\n\n客户端错误：\n==400      错误请求，如语法错误==401      未授权HTTP 401.1    未授权，登录失败HTTP 401.2    未授权，服务器配置问题导致登录失败HTTP 401.3    ACL  禁止访问资源HTTP 401.4    未授权  授权被筛选器拒绝HTTP 401.5    未授权  ISAPI或CGI授权失败402      保留有效ChargeTo头响应==403      禁止访问==HTTP 403.1    禁止访问  禁止可执行访问HTTP 403.2    禁止访问  禁止读访问HTTP 403.3    禁止访问  禁止写访问HTTP 403.4    禁止访问  要求SSLHTTP 403.5    禁止访问  要求SSL 128HTTP 403.6    禁止访问  IP地址被拒绝HTTP 403.7    禁止访问  要求客户端证书HTTP 403.8    禁止访问  禁止站点访问HTTP 403.9    禁止访问  连接的用户过多HTTP 403.10   禁止访问  配置无效HTTP 403.11   禁止访问  密码更改HTTP 403.12   禁止访问  映射器拒绝访问HTTP 403.13   禁止访问  客户端证书已被吊销HTTP 403.15   禁止访问  客户端访问许可过多HTTP 403.16   禁止访问  客户端证书不可信或者无效HTTP 403.17   禁止访问  客户端证书已经到期或者尚未生效==404       没有发现文件、查询或URL==405       用户在Request-Line字段定义的方法不允许406       根据用户发送的Accept拖，请求资源不可访问407       类似401，用户必须首先在代理服务器上得到授权408       客户端没有在用户指定的饿时间内完成请求409       对当前资源状态，请求不能完成410       服务器上不再有此资源且无进一步的参考地址411       服务器拒绝用户定义的Content-Length属性请求 412       一个或多个请求头字段在当前请求中错误413       请求的资源大于服务器允许的大小414       请求的资源URL长于服务器允许的长度415       请求资源不支持请求项目格式416       请求中包含Range请求头字段，在当前请求资源范围内没有range指示值，       请求也不包含If-Range请求头字段417       服务器不满足请求Expect头字段指定的期望值，如果是代理服务器，可能是下一级服务器不能满足请求长\n\n服务器端错误:\n==500 - 内部服务器错误==HTTP 500.100       内部服务器错误 HTTP 500-11       服务器关闭HTTP 500-12       应用程序重新启动HTTP 500-13       服务器太忙HTTP 500-14       应用程序无效HTTP 500-15       不允许请求 501       未实现502       网关错误503       服务不可用504       网关超时\n\n请求头参数在http协议中，向服务器发送一个请求，数据分为三部分，第一个会把数据放在URL中，第二个是在body中（post请求），第三个是放在head中，head即为请求头，爬虫常用请求头参数如下：\n\n\n\nHeader\n解释\n示例\n\n\n\nCookie\nHTTP请求发送时，会把保存在该请求域名下的所有cookie值一起发送给web服务器。\nCookie: $Version=1; Skin=new;\n\n\nReferer\n先前网页的地址，当前请求网页紧随其后，即来路\nReferer: http://www.example.com/archives…\n\n\nUser-Agent\nUser-Agent的内容包含发出请求的用户信息,包含浏览器名称\nUser-Agent: Mozilla/5.0 (Linux; X11)\n\n\n浏览器开发者工具chrome浏览器的开发者工具可以在右键→检查调出，也可以f12或者Ctrl+Shift+I（进入开发者工具）Ctrl+Shift+J （进入开发者工具，并定位到控制台Console）\n开发者工具简要说明：\nElements 元素，显示网页源码\nConsule 控制台，跟踪查看js代码\nSources 显示网页文件\nNetwork 查看网页请求\n\n示例\n如图是开发者工具的界面，红框标注的是上文提到的http请求的相关参数。\n爬虫使用时的法律要求法律方面参考《数据安全管理办法（征求意见稿）》，总之就是禁止恶意爬虫，所谓恶意爬虫就是涉及到以下方面\n\n侵犯著作权\n侵犯商业秘密\n侵犯个人隐私或个人信息\n构成不正当竞争\n侵入计算机系统，构成刑事犯罪\n\n  同时遵守Robots协议，robots.txt协议并不是一个规范，而是约定俗成的东西，网站通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取：根据协议，网站管理员可以在网站域名的根目录下放一个robots.txt 文本文件，里面可以指定不同的网络爬虫能访问的页面和禁止访问的页面，指定的页面由正则表达式表示。网络爬虫在采集这个网站之前，首先获取到这个文件，然后解析到其中的规则，然后根据规则来采集网站的数据。\nTODO⛳到这里学习爬虫的铺垫就打好了，后面就从python中与爬虫相关的库开始学起\n\n   \n\n\n\n\n参考：[1] 21天搞定Python分布爬虫[2] 通俗的讲，网络爬虫到底是什么？.史中[3] HTTP协议超级详解.爱文飞翔[4] URL的组成格式.wudipmd[5] HTTP 请求方法.菜鸟教程[6] Apache之HTTP协议[7] Header:请求头参数详解.brucelong[8] 国家互联网信息办公室关于《数据安全管理办法（征求意见稿）》公开征求意见的通知[9] 网络爬虫的法律规制.2019.06.16\n","categories":["python"],"tags":["网络爬虫","python","http协议"]}]